{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLcrthSfUkHYeO1sYFoEgt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jacofeldman/Metodos1_JacoboFeldman/blob/main/Parcial2/Soluci%C3%B3n_num%C3%A9rica_27.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# a)\n",
        "funciones = (\n",
        "    lambda w0, w1, w2, w3, x0, x1, x2, x3: w0 + w1 + w2 + w3 - 2,\n",
        "    lambda w0, w1, w2, w3, x0, x1, x2, x3: w0 * x0 + w1 * x1 + w2 * x2 + w3 * x3,\n",
        "    lambda w0, w1, w2, w3, x0, x1, x2, x3: w0 * x0**2 + w1 * x1**2 + w2 * x2**2 + w3 * x3**2 - 2/3,\n",
        "    lambda w0, w1, w2, w3, x0, x1, x2, x3: w0 * x0**3 + w1 * x1**3 + w2 * x2**3 + w3 * x3**3,\n",
        "    lambda w0, w1, w2, w3, x0, x1, x2, x3: w0 * x0**4 + w1 * x1**4 + w2 * x2**4 + w3 * x3**4 - 2/5,\n",
        "    lambda w0, w1, w2, w3, x0, x1, x2, x3: w0 * x0**5 + w1 * x1**5 + w2 * x2**5 + w3 * x3**5,\n",
        "    lambda w0, w1, w2, w3, x0, x1, x2, x3: w0 * x0**6 + w1 * x1**6 + w2 * x2**6 + w3 * x3**6 - 2/7,\n",
        "    lambda w0, w1, w2, w3, x0, x1, x2, x3: w0 * x0**7 + w1 * x1**7 + w2 * x2**7 + w3 * x3**7\n",
        ")\n",
        "\n",
        "# b)\n",
        "def F(x):\n",
        "    w0, w1, w2, w3 = x[:4]\n",
        "    x0, x1, x2, x3 = x[4:]\n",
        "    return np.array([f(w0, w1, w2, w3, x0, x1, x2, x3) for f in funciones])\n",
        "\n",
        "# c)\n",
        "def jacobiano(F, x, h=1e-6):\n",
        "    n = len(x)\n",
        "    J = np.zeros((n, n))\n",
        "    Fx = F(x)\n",
        "    for i in range(n):\n",
        "        x_h = np.copy(x)\n",
        "        x_h[i] += h\n",
        "        J[:, i] = (F(x_h) - Fx) / h\n",
        "    return J\n",
        "\n",
        "# d)\n",
        "def descenso_gradiente(F, x0, gamma=0.01, tol=0.005):\n",
        "    x = np.copy(x0)\n",
        "    error = np.linalg.norm(F(x))\n",
        "    iteration = 0  # Contador de iteraciones\n",
        "    while error > tol:\n",
        "        J = jacobiano(F, x)\n",
        "        gradiente = np.linalg.solve(J, F(x))\n",
        "        x -= gamma * gradiente\n",
        "        error = np.linalg.norm(F(x))\n",
        "        iteration += 1  # Incrementar el contador de iteraciones\n",
        "\n",
        "        # Imprimir el error y la tasa de aprendizaje cada 100 iteraciones\n",
        "        if iteration % 100 == 0:\n",
        "          #e)\n",
        "            if error < 0.005:  # Ajuste de la tasa de aprendizaje cuando el error disminuye\n",
        "                gamma = 0.001\n",
        "            print(f\"Iteración {iteration}: Error: {error:.6f}, Tasa de aprendizaje: {gamma}\")\n",
        "\n",
        "    return x\n",
        "\n",
        "# Generar una semilla aleatoria para los 8 valores (4 pesos y 4 puntos)\n",
        "x0 = np.random.uniform(-1., 1., size=8)\n",
        "\n",
        "# Ejecutar el descenso de gradiente para resolver las ecuaciones\n",
        "solucion = descenso_gradiente(F, x0)\n",
        "\n",
        "# Mostrar la solución final\n",
        "w0, w1, w2, w3 = solucion[:4]\n",
        "x0, x1, x2, x3 = solucion[4:]\n",
        "print(f\"Pesos: w0 = {w0:.6f}, w1 = {w1:.6f}, w2 = {w2:.6f}, w3 = {w3:.6f}\")\n",
        "print(f\"Puntos de Gauss: x0 = {x0:.6f}, x1 = {x1:.6f}, x2 = {x2:.6f}, x3 = {x3:.6f}\")\n",
        "\n",
        "#f)\n",
        "def calcular_integral(w, x):\n",
        "    integral = sum(w[i] * np.cos(x[i]) for i in range(4))\n",
        "    return integral\n",
        "def f(x):\n",
        "    return np.cos(x)\n",
        "\n",
        "integral_real=2 * np.sin(1)\n",
        "resultado_integral = calcular_integral(solucion[:4], solucion[4:])\n",
        "print(f\"Integral real: {resultado_integral:.8f}\")\n",
        "print(f\"Estimación de la integral: {integral_real:.8f}\")\n",
        "print(f\"Error del metodo: {abs(integral_real - resultado_integral):.8f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMXClAV0aHYp",
        "outputId": "0aa84994-6b87-4af9-c262-c16897f0516d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 100: Error: 1418.203567, Tasa de aprendizaje: 0.01\n",
            "Iteración 200: Error: 519.586179, Tasa de aprendizaje: 0.01\n",
            "Iteración 300: Error: 190.196461, Tasa de aprendizaje: 0.01\n",
            "Iteración 400: Error: 69.881074, Tasa de aprendizaje: 0.01\n",
            "Iteración 500: Error: 33.695144, Tasa de aprendizaje: 0.01\n",
            "Iteración 600: Error: 12.279581, Tasa de aprendizaje: 0.01\n",
            "Iteración 700: Error: 4.486760, Tasa de aprendizaje: 0.01\n",
            "Iteración 800: Error: 1.641832, Tasa de aprendizaje: 0.01\n",
            "Iteración 900: Error: 37092.803233, Tasa de aprendizaje: 0.01\n",
            "Iteración 1000: Error: 13577.095475, Tasa de aprendizaje: 0.01\n",
            "Iteración 1100: Error: 4969.633367, Tasa de aprendizaje: 0.01\n",
            "Iteración 1200: Error: 1819.036275, Tasa de aprendizaje: 0.01\n",
            "Iteración 1300: Error: 665.818900, Tasa de aprendizaje: 0.01\n",
            "Iteración 1400: Error: 243.704652, Tasa de aprendizaje: 0.01\n",
            "Iteración 1500: Error: 89.197269, Tasa de aprendizaje: 0.01\n",
            "Iteración 1600: Error: 32.642519, Tasa de aprendizaje: 0.01\n",
            "Iteración 1700: Error: 11.941469, Tasa de aprendizaje: 0.01\n",
            "Iteración 1800: Error: 4.363690, Tasa de aprendizaje: 0.01\n",
            "Iteración 1900: Error: 1.588356, Tasa de aprendizaje: 0.01\n",
            "Iteración 2000: Error: 0.566043, Tasa de aprendizaje: 0.01\n",
            "Iteración 2100: Error: 0.246266, Tasa de aprendizaje: 0.01\n",
            "Iteración 2200: Error: 0.040450, Tasa de aprendizaje: 0.01\n",
            "Iteración 2300: Error: 0.018166, Tasa de aprendizaje: 0.01\n",
            "Iteración 2400: Error: 0.006648, Tasa de aprendizaje: 0.01\n",
            "Pesos: w0 = 0.626674, w1 = 0.351011, w2 = 0.367104, w3 = 0.655210\n",
            "Puntos de Gauss: x0 = -0.349451, x1 = -0.856737, x2 = 0.852532, x3 = 0.315779\n",
            "Integral real: 1.68307488\n",
            "Estimación de la integral: 1.68294197\n",
            "Error del metodo: 0.00013291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "g)La precisión de la estimación en el método de descenso de gradiente puede verse afectada por varias razones. Una de las principales es la presencia de mínimos locales, que puede atrapar al algoritmo en soluciones subóptimas. La elección de la semilla inicial también es crucial, ya que diferentes valores pueden llevar a diferentes resultados. Además, una tasa de aprendizaje inapropiada puede causar oscilaciones o lentitud en la convergencia. La precisión del Jacobiano y el gradiente, calculados mediante diferencias finitas, también depende del valor de\n",
        "ℎ\n",
        ", y si las funciones son altamente no lineales, el algoritmo puede tener dificultades para encontrar el mínimo correcto."
      ],
      "metadata": {
        "id": "WRpYLoDpbvm2"
      }
    }
  ]
}