{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jacofeldman/Metodos1_JacoboFeldman/blob/main/tarea4/Punto12_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPtxwJkTsv4P",
        "outputId": "8d778685-5f91-446a-cb37-b66ace90b086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged in 5 iterations\n",
            "Solución usando Newton-Raphson: [1.77245385 1.77245385]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Definir las funciones corregidas\n",
        "def F(x):\n",
        "    x1, x2 = x\n",
        "    f1 = np.log(x1**2 + x2**2) - np.sin(x1 * x2) - (np.log(2) + np.log(np.pi))\n",
        "    f2 = np.exp(x1 - x2) + np.cos(x1 * x2)\n",
        "    return np.array([f1, f2])\n",
        "\n",
        "# Definir la Jacobiana corregida\n",
        "def J(x):\n",
        "    x1, x2 = x\n",
        "    df1_dx1 = (2 * x1) / (x1**2 + x2**2) - x2 * np.cos(x1 * x2)\n",
        "    df1_dx2 = (2 * x2) / (x1**2 + x2**2) - x1 * np.cos(x1 * x2)\n",
        "    df2_dx1 = np.exp(x1 - x2) - x2 * np.sin(x1 * x2)\n",
        "    df2_dx2 = -np.exp(x1 - x2) - x1 * np.sin(x1 * x2)\n",
        "    return np.array([[df1_dx1, df1_dx2], [df2_dx1, df2_dx2]])\n",
        "\n",
        "# Newton-Raphson corregido\n",
        "def newton_raphson(F, J, x0, tol=1e-6, max_iter=100):\n",
        "    x = np.array(x0, dtype=float)\n",
        "    for i in range(max_iter):\n",
        "        Fx = F(x)\n",
        "        if np.linalg.norm(Fx) < tol:\n",
        "            print(f\"Converged in {i} iterations\")\n",
        "            return x\n",
        "        Jx = J(x)\n",
        "        delta_x = np.linalg.solve(Jx, -Fx)\n",
        "        x = x + delta_x\n",
        "    raise ValueError(\"No convergence after maximum iterations\")\n",
        "\n",
        "# Inicialización\n",
        "x0 = [2, 2]\n",
        "\n",
        "# Solución usando Newton-Raphson\n",
        "solution_nr = newton_raphson(F, J, x0)\n",
        "print(\"Solución usando Newton-Raphson:\", solution_nr)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Función objetivo corregida\n",
        "def objective(x):\n",
        "    f = F(x)\n",
        "    return 0.5 * np.sum(f**2)\n",
        "\n",
        "# Gradiente de la función objetivo corregida\n",
        "def gradient(x):\n",
        "    Jx = J(x)\n",
        "    Fx = F(x)\n",
        "    return np.dot(Jx.T, Fx)\n",
        "\n",
        "# Descenso de gradiente corregido\n",
        "def gradient_descent(objective, gradient, x0, lr=0.01, tol=1e-6, max_iter=1000):\n",
        "    x = np.array(x0, dtype=float)\n",
        "    for i in range(max_iter):\n",
        "        grad = gradient(x)\n",
        "        if np.linalg.norm(grad) < tol:\n",
        "            print(f\"Converged in {i} iterations\")\n",
        "            return x\n",
        "        x = x - lr * grad\n",
        "    raise ValueError(\"No convergence after maximum iterations\")\n",
        "\n",
        "# Solución usando descenso de gradiente\n",
        "solution_gd = gradient_descent(objective, gradient, x0)\n",
        "print(\"Solución usando descenso de gradiente:\", solution_gd)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oUJfbtNiZge",
        "outputId": "15ca6e0c-8e06-4b3a-dc67-66725955cdd0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged in 543 iterations\n",
            "Solución usando descenso de gradiente: [1.7724535 1.7724542]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Definir el valor de q\n",
        "q = 1\n",
        "\n",
        "# Definir las funciones del sistema con la corrección\n",
        "def F2(x):\n",
        "    x1, x2, x3 = x\n",
        "    f1 = 6*x1 - 2*np.cos(x2 * x3) - 1\n",
        "    f2 = 9*x2 - np.sqrt(x1**2 + np.sin(x3) + 1.06) + 0.9\n",
        "    f3 = 60*x3 + 3*np.exp(-x1 * x2) + 10*np.pi - 3\n",
        "    return np.array([f1, f2, f3])\n",
        "\n",
        "# Definir la Jacobiana\n",
        "def J2(x):\n",
        "    x1, x2, x3 = x\n",
        "    df1_dx1 = 6\n",
        "    df1_dx2 = 2 * x3 * np.sin(x2 * x3)\n",
        "    df1_dx3 = 2 * x2 * np.sin(x2 * x3)\n",
        "\n",
        "    df2_dx1 = x1 / np.sqrt(x1**2 + np.sin(x3) + 1.06)\n",
        "    df2_dx2 = 9\n",
        "    df2_dx3 = np.cos(x3)\n",
        "\n",
        "    df3_dx1 = -3 * x2 * np.exp(-x1 * x2)\n",
        "    df3_dx2 = -3 * x1 * np.exp(-x1 * x2)\n",
        "    df3_dx3 = 60\n",
        "\n",
        "    return np.array([\n",
        "        [df1_dx1, df1_dx2, df1_dx3],\n",
        "        [df2_dx1, df2_dx2, df2_dx3],\n",
        "        [df3_dx1, df3_dx2, df3_dx3]\n",
        "    ])\n",
        "\n",
        "# Método de Newton-Raphson\n",
        "def newton_raphson_2(F, J, x0, tol=1e-6, max_iter=100):\n",
        "    x = np.array(x0, dtype=float)\n",
        "    for i in range(max_iter):\n",
        "        Fx = F(x)\n",
        "        if np.linalg.norm(Fx) < tol:\n",
        "            print(f\"Converged in {i} iterations\")\n",
        "            return x\n",
        "        Jx = J(x)\n",
        "        delta_x = np.linalg.solve(Jx, -Fx)\n",
        "        x = x + delta_x\n",
        "    raise ValueError(\"No convergence after maximum iterations\")\n",
        "\n",
        "# Inicialización\n",
        "x0_2 = [0, 0, 0]\n",
        "\n",
        "# Solución usando Newton-Raphson\n",
        "solution_nr_2 = newton_raphson_2(F2, J2, x0_2)\n",
        "print(\"Solución usando Newton-Raphson para el segundo sistema:\", solution_nr_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LEJI4hHlWJL",
        "outputId": "545122c9-185e-4861-ad7a-3da4b306b7e8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged in 5 iterations\n",
            "Solución usando Newton-Raphson para el segundo sistema: [ 5.00000000e-01  4.89114339e-10 -5.23598776e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Función objetivo para el segundo sistema\n",
        "def objective_2(x):\n",
        "    f = F2(x)\n",
        "    return 0.5 * np.sum(f**2)\n",
        "\n",
        "# Gradiente del segundo sistema\n",
        "def gradient_2(x):\n",
        "    Jx = J2(x)\n",
        "    Fx = F2(x)\n",
        "    return np.dot(Jx.T, Fx)\n",
        "\n",
        "# Descenso de gradiente para el segundo sistema\n",
        "def gradient_descent_2(objective, gradient, x0, lr=0.01, tol=1e-6, max_iter=100000):\n",
        "    x = np.array(x0, dtype=float)\n",
        "    for i in range(max_iter):\n",
        "        grad = gradient(x)\n",
        "        if np.linalg.norm(grad) < tol:\n",
        "            print(f\"Converged in {i} iterations\")\n",
        "            return x\n",
        "        x = x - lr * grad\n",
        "    raise ValueError(\"No convergence after maximum iterations\")\n",
        "\n",
        "# Solución usando descenso de gradiente\n",
        "solution_gd_2 = gradient_descent_2(objective_2, gradient_2, x0_2)\n",
        "print(\"Solución usando descenso de gradiente para el segundo sistema:\", solution_gd_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "PcpbWofBlafZ",
        "outputId": "c1ecd713-4671-496c-94fd-da0246c3c91d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-96a32a7b267b>:25: RuntimeWarning: overflow encountered in exp\n",
            "  df3_dx1 = -3 * x2 * np.exp(-x1 * x2)\n",
            "<ipython-input-9-96a32a7b267b>:26: RuntimeWarning: overflow encountered in exp\n",
            "  df3_dx2 = -3 * x1 * np.exp(-x1 * x2)\n",
            "<ipython-input-9-96a32a7b267b>:11: RuntimeWarning: overflow encountered in exp\n",
            "  f3 = 60*x3 + 3*np.exp(-x1 * x2) + 10*np.pi - 3\n",
            "<ipython-input-9-96a32a7b267b>:18: RuntimeWarning: invalid value encountered in sin\n",
            "  df1_dx2 = 2 * x3 * np.sin(x2 * x3)\n",
            "<ipython-input-9-96a32a7b267b>:19: RuntimeWarning: invalid value encountered in sin\n",
            "  df1_dx3 = 2 * x2 * np.sin(x2 * x3)\n",
            "<ipython-input-9-96a32a7b267b>:21: RuntimeWarning: invalid value encountered in sin\n",
            "  df2_dx1 = x1 / np.sqrt(x1**2 + np.sin(x3) + 1.06)\n",
            "<ipython-input-9-96a32a7b267b>:23: RuntimeWarning: invalid value encountered in cos\n",
            "  df2_dx3 = np.cos(x3)\n",
            "<ipython-input-9-96a32a7b267b>:9: RuntimeWarning: invalid value encountered in cos\n",
            "  f1 = 6*x1 - 2*np.cos(x2 * x3) - 1\n",
            "<ipython-input-9-96a32a7b267b>:10: RuntimeWarning: invalid value encountered in sin\n",
            "  f2 = 9*x2 - np.sqrt(x1**2 + np.sin(x3) + 1.06) + 0.9\n",
            "<ipython-input-9-96a32a7b267b>:11: RuntimeWarning: invalid value encountered in scalar add\n",
            "  f3 = 60*x3 + 3*np.exp(-x1 * x2) + 10*np.pi - 3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No convergence after maximum iterations",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-8150e9c867e7>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Solución usando descenso de gradiente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0msolution_gd_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Solución usando descenso de gradiente para el segundo sistema:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolution_gd_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-8150e9c867e7>\u001b[0m in \u001b[0;36mgradient_descent_2\u001b[0;34m(objective, gradient, x0, lr, tol, max_iter)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No convergence after maximum iterations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Solución usando descenso de gradiente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No convergence after maximum iterations"
          ]
        }
      ]
    }
  ]
}